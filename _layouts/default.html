<!DOCTYPE html>
<html lang="{{ site.lang | default: 'en-US' }}">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="{{ '/assets/css/style.css?v=' | append: site.github.build_revision | relative_url }}"
    />
    {% seo %}
    {% include head-custom.html %}
    <title>Recipe For a Five-Star Rating</title>
  </head>

  <body>
    <div id="header_wrap" class="outer">
      <header class="inner">
        {% if site.github.is_project_page %}
          <div class="github-banner">
            <a id="forkme_banner" href="{{ site.github.repository_url }}">View on GitHub</a>
          </div>
        {% endif %}
        <h1 id="project_title">Recipe For a Five-Star Rating</h1>
        <h2 id="project_tagline">Can nutrition, steps, and timing predict a 5-star meal?</h2>
        <p><strong>By Nadia Sau – nadia@email.com</strong></p>
      </header>
    </div>

    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner" style="max-width: 1000px; margin: 0 auto">

        <h2>Introduction</h2>
        <p>
          This project investigates what makes a recipe highly rated on Food.com. By analyzing thousands of user-submitted recipes and their ratings, the goal is to predict a recipe's average rating based on metadata like preparation time, number of steps, and nutrition. I used exploratory data analysis, feature engineering, and machine learning to build a model that predicts recipe ratings.
        </p>

        <!-- Dataset Tables -->
        <div style="display: flex; gap: 40px; flex-wrap: wrap;">
          <div style="flex: 1;">
            <h3>Recipes</h3>
            <table>
              <thead>
                <tr><th>Column</th><th>Description</th></tr>
              </thead>
              <tbody>
                <tr><td>name</td><td>Recipe name</td></tr>
                <tr><td>id</td><td>Recipe ID</td></tr>
                <tr><td>minutes</td><td>Preparation time</td></tr>
                <tr><td>contributor_id</td><td>User who submitted it</td></tr>
                <tr><td>submitted</td><td>Date submitted</td></tr>
                <tr><td>tags</td><td>Associated tags</td></tr>
                <tr><td>nutrition</td><td>List: [calories, fat, sugar, sodium, protein, sat. fat, carbs]</td></tr>
                <tr><td>n_steps</td><td>Number of steps</td></tr>
                <tr><td>steps</td><td>Step-by-step text</td></tr>
                <tr><td>description</td><td>Recipe description</td></tr>
              </tbody>
            </table>
          </div>

          <div style="flex: 1;">
            <h3>Ratings</h3>
            <table>
              <thead>
                <tr><th>Column</th><th>Description</th></tr>
              </thead>
              <tbody>
                <tr><td>user_id</td><td>User ID</td></tr>
                <tr><td>recipe_id</td><td>Linked recipe ID</td></tr>
                <tr><td>date</td><td>Rating date</td></tr>
                <tr><td>rating</td><td>1–5 rating</td></tr>
                <tr><td>review</td><td>Optional review text</td></tr>
              </tbody>
            </table>
          </div>
        </div>

        <h2>Data Cleaning and Exploratory Data Analysis</h2>
        <p>
          To explore and prepare the dataset for analysis, I first performed several cleaning and transformation steps to ensure the data was usable and meaningful for evaluating recipe ratings.
        </p>

        <h3>Merging Datasets</h3>
        <p>
          I began by merging the <code>Recipes</code> and <code>Ratings</code> datasets into a single DataFrame. Since each recipe can have multiple ratings, I performed a left merge using the <code>id</code> column from the <code>Recipes</code> table and the <code>recipe_id</code> column from the <code>Ratings</code> table.
        </p>

        <h3>Average Rating</h3>
        <p>
          Because each recipe has multiple user-submitted ratings, I calculated the average rating for each recipe using <code>groupby</code> on <code>recipe_id</code> and taking the mean of the <code>rating</code> values. I then mapped the resulting Series back to the full DataFrame using the <code>id</code> column. Before this, I replaced any rating values of 0 with <code>NaN</code> to avoid skewing the average.
        </p>

        <h3>Review Count</h3>
        <p>
          I was curious whether the number of reviews a recipe received correlated with its average rating. To explore this, I created a <code>review_count</code> column by using <code>value_counts()</code> on the <code>recipe_id</code> column and then mapping those counts back to the full dataset using the <code>id</code> column. I also filled any missing values with 0.
        </p>

        <h3>Health Score</h3>
        <p>
          To examine whether a recipe's healthiness impacted its rating, I created a <code>health_score</code> based on the nutritional data provided. I extracted individual nutritional values into separate columns (e.g., sugar, protein, sodium), assumed each recipe serves four people, and adjusted the values accordingly.
        </p>
        <p>
          I then applied a weighting system to reflect the nutritional value of each component:
        </p>
        <ul>
          <li><strong>Positive weight</strong>: Protein (+2)</li>
          <li><strong>Negative weights</strong>: Sugar (-1.8), saturated fat (-1.5), total fat, carbs, calories (-1 each), and sodium (-0.5)</li>
        </ul>
        <p>
          The resulting <code>health_score</code> was negative for healthier recipes and positive for less healthy ones, so I flipped the sign so that <strong>higher scores indicate unhealthier recipes</strong>.
        </p>
        <p>
          Initially, some scores were unrealistically high (e.g., over 100,000), due to outliers such as a single recipe having a sugar PDV of 30,000. To fix this, I clipped extreme health score values between the 1st and 98th percentile. This reduced the total number of recipes from ~23,000 to ~22,000, the mean score from 62 to 49, and the standard deviation from 147 to 48. The maximum score also dropped from over 16,000 to 329.
        </p>

        <h3>Recipe Duration (Minutes)</h3>
        <p>
          Lastly, I addressed outliers in the <code>minutes</code> column, which included some recipes that took months to prepare. I clipped the top and bottom 1% of the <code>minutes</code> distribution to focus on recipes with realistic preparation times.
        </p>
        <p>
          With the data cleaned and key features engineered, I created several visualizations to explore how different factors influence recipe ratings. I found the strongest correlations with health score, number of ingredients, number of reviews, and number of steps.
        </p>

        <h3>Exploratory Visualizations</h3>
        <div style="display: flex; flex-wrap: wrap; gap: 20px; justify-content: center;">
          <iframe src="assets/health-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          <iframe src="assets/ingNumber-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          <iframe src="assets/numReviews-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          <iframe src="assets/steps-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
        </div>

        <h2>Framing a Prediction Problem</h2>
        <p>
          <strong>Prediction Problem:</strong> Can we predict the average rating of a recipe based on its number of ingredients, number of steps, review count, and health score?
        </p>
        <p>
          <strong>Type of Prediction:</strong> Regression (predicting a continuous value from 1 to 5)
        </p>
        <p>

        <h2>Baseline Model</h2>
        <p>
          I first used a <strong>Linear Regression</strong> model as a baseline. These included:
        </p>
        <ul>
          <li>Number of ingredients (<code>n_ingredients</code>)</li>
          <li>Number of steps (<code>n_steps</code>)</li>
          <li>Health score (<code>health_score</code>)</li>
          <li>Review count (<code>review_count</code>)</li>
        </ul>
        <p>
          The baseline model showed:
        </p>
        <ul>
          <li>Mean Squared Error (MSE): <strong>0.2484</strong></li>
          <li>R² Score: <strong>0.003</strong></li>
        </ul>
        <p>This indicated a weak linear fit — more complex models were needed.</p>

        <h2>Final Model</h2>

        <h3>Feature Engineering</h3>
        <p>
          In the final model, I standardized all numeric features using <code>StandardScaler</code> within a <code>ColumnTransformer</code> to ensure each feature contributed equally to the model's predictions.
        </p>
        
        <h3>Model Selection</h3>
        <p>
          I used a <strong>Random Forest Regressor</strong> to capture complex, non-linear relationships in the data. I tuned the model using <code>GridSearchCV</code> across the following hyperparameters:
        </p>
        <ul>
          <li><strong>n_estimators:</strong> [300, 400] — number of trees in the forest</li>
          <li><strong>max_depth:</strong> [30, 40] — maximum depth of each tree</li>
        </ul>
        <p>
          The best-performing model used <strong>400 estimators</strong> and a <strong>maximum depth of 40</strong>. Since the optimal parameters were at the upper limit of the tested range, additional tuning might yield marginal improvements. However, performance gains appeared to taper off beyond a depth of 30, indicating the model was approaching its optimal complexity.
        </p>
        
        <h3>Performance Comparison</h3>
        <p><strong>Baseline Model:</strong> Linear Regression with original numeric features</p>
        <ul>
          <li>MSE: <strong>0.248</strong></li>
          <li>R² Score: <strong>0.003</strong></li>
        </ul>
        
        <p><strong>Final Model:</strong> Random Forest with scaled numeric features</p>
        <ul>
          <li>MSE: <strong>0.136</strong></li>
          <li>R² Score: <strong>0.453</strong></li>
        </ul>
        
        <h3>Improvement</h3>
        <p>
          The final model showed a substantial improvement in both <strong>mean squared error</strong> and <strong>R² score</strong>, capturing significantly more variance in recipe ratings than the baseline. The Random Forest’s flexibility, combined with feature scaling and thorough hyperparameter tuning, led to a much more accurate prediction model.
        </p>

        <p>See full code and analysis on <a href="{{ site.github.repository_url }}">GitHub</a>.</p>
      </section>
    </div>
  </body>
</html>
