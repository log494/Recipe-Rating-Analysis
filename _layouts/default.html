<!DOCTYPE html>
<html lang="{{ site.lang | default: 'en-US' }}">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="{{ '/assets/css/style.css?v=' | append: site.github.build_revision | relative_url }}"
    />
    {% seo %}
    {% include head-custom.html %}
    <title>Recipe For a Five-Star Rating</title>
  </head>

  <body>
    <div id="header_wrap" class="outer">
      <header class="inner">
        {% if site.github.is_project_page %}
          <div class="github-banner">
            <a id="forkme_banner" href="{{ site.github.repository_url }}">View on GitHub</a>
          </div>
        {% endif %}
        <h1 id="project_title">Recipe For a Five-Star Rating</h1>
        <h2 id="project_tagline">Can nutrition, steps, and timing predict a 5-star meal?</h2>
        <p><strong>By Nadia Sau – nadia@email.com</strong></p>
      </header>
    </div>

    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner" style="max-width: 1000px; margin: 0 auto">
        
        <h2>Introduction</h2>
        <p>
          This project investigates what makes a recipe highly rated on Food.com. By analyzing thousands of user-submitted recipes and their ratings, the goal is to predict a recipe's average rating based on metadata like preparation time, number of steps, and nutrition. I used exploratory data analysis, feature engineering, and machine learning to build a model that predicts recipe ratings.
        </p>

        <!-- Dataset Tables -->
        <div style="display: flex; gap: 40px; flex-wrap: wrap;">
          <div style="flex: 1;">
            <h3>Recipes</h3>
            <table>
              <thead>
                <tr><th>Column</th><th>Description</th></tr>
              </thead>
              <tbody>
                <tr><td>name</td><td>Recipe name</td></tr>
                <tr><td>id</td><td>Recipe ID</td></tr>
                <tr><td>minutes</td><td>Preparation time</td></tr>
                <tr><td>contributor_id</td><td>User who submitted it</td></tr>
                <tr><td>submitted</td><td>Date submitted</td></tr>
                <tr><td>tags</td><td>Associated tags</td></tr>
                <tr><td>nutrition</td><td>List: [calories, fat, sugar, sodium, protein, sat. fat, carbs]</td></tr>
                <tr><td>n_steps</td><td>Number of steps</td></tr>
                <tr><td>steps</td><td>Step-by-step text</td></tr>
                <tr><td>description</td><td>Recipe description</td></tr>
              </tbody>
            </table>
          </div>

          <div style="flex: 1;">
            <h3>Ratings</h3>
            <table>
              <thead>
                <tr><th>Column</th><th>Description</th></tr>
              </thead>
              <tbody>
                <tr><td>user_id</td><td>User ID</td></tr>
                <tr><td>recipe_id</td><td>Linked recipe ID</td></tr>
                <tr><td>date</td><td>Rating date</td></tr>
                <tr><td>rating</td><td>1–5 rating</td></tr>
                <tr><td>review</td><td>Optional review text</td></tr>
              </tbody>
            </table>
          </div>
        </div>

        <h2>Data Cleaning and Exploratory Data Analysis</h2>
      
        <p>
            To explore and prepare the dataset for analysis, I first performed several cleaning and transformation steps to ensure the data was usable and meaningful for evaluating recipe ratings.
        </p>
       
        <h3>Merging Datasets</h3>
        <p>
            I began by merging the <code>Recipes</code> and <code>Ratings</code> datasets into a single DataFrame. Since each recipe can have multiple ratings, I performed a left merge using the <code>id</code> column from the <code>Recipes</code> table and the <code>recipe_id</code> column from the <code>Ratings</code> table.
        </p>
       
        <h3>Average Rating</h3>
        <p>
            Because each recipe has multiple user-submitted ratings, I calculated the average rating for each recipe using <code>groupby</code> on <code>recipe_id</code> and taking the mean of the <code>rating</code> values. I then mapped the resulting Series back to the full DataFrame using the <code>id</code> column. Before this, I replaced any rating values of 0 with <code>NaN</code> to avoid skewing the average.
        </p>
       
        <h3>Review Count</h3>
        <p>
            I was curious whether the number of reviews a recipe received correlated with its average rating. To explore this, I created a <code>review_count</code> column by using <code>value_counts()</code> on the <code>recipe_id</code> column and then mapping those counts back to the full dataset using the <code>id</code> column. I also filled any missing values with 0.
        </p>
       
        <h3>Health Score</h3>
        <p>
            To examine whether a recipe's healthiness impacted its rating, I created a <code>health_score</code> based on the nutritional data provided. I extracted individual nutritional values into separate columns (e.g., sugar, protein, sodium), assumed each recipe serves four people, and adjusted the values accordingly.
        </p>
        <p>
            I then applied a weighting system to reflect the nutritional value of each component:
        </p>
        <ul>
            <li><strong>Positive weight</strong>: Protein (+2)</li>
            <li><strong>Negative weights</strong>: Sugar (-1.8), saturated fat (-1.5), total fat, carbs, calories (-1 each), and sodium (-0.5)</li>
        </ul>
        <p>
            The resulting <code>health_score</code> was negative for healthier recipes and positive for less healthy ones, so I flipped the sign so that <strong>higher scores indicate unhealthier recipes</strong>.
        </p>
        <p>
            Initially, some scores were unrealistically high (e.g., over 100,000), due to outliers such as a single recipe having a sugar PDV of 30,000. To fix this, I clipped extreme health score values between the 1st and 98th percentile. This reduced the total number of recipes from ~23,000 to ~22,000, the mean score from 62 to 49, and the standard deviation from 147 to 48. The maximum score also dropped from over 16,000 to 329.
        </p>
       
        <h3>Recipe Duration (Minutes)</h3>
        <p>
            Lastly, I addressed outliers in the <code>minutes</code> column, which included some recipes that took months to prepare. I clipped the top and bottom 1% of the <code>minutes</code> distribution to focus on recipes with realistic preparation times.
        </p>
        <p>
          With the data cleaned and key features engineered, I created several visualizations to explore how different factors influence recipe ratings. I found the strongest correlations with health score, number of ingredients, number of reviews, and number of steps        </p>
          <h3>Exploratory Visualizations</h3>
          <div style="display: flex; flex-wrap: wrap; gap: 20px; justify-content: center;">
            <iframe src="assets/health-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          
            <iframe src="assets/ingNumber-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          
            <iframe src="assets/numReviews-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          
            <iframe src="assets/steps-vs-rating.html" width="800" height="600" frameborder="0"></iframe>
          
          </div>

        <h2>Framing a Prediction Problem</h2>
        <p>
          <strong>Prediction Problem:</strong> Can we predict the average rating of a recipe based on its preparation time, number of steps, review count, and health score?
        </p>
        <p>
          <strong>Type of Prediction:</strong> Regression (predicting a continuous value from 1 to 5)
        </p>
        <p>
          <strong>Features Used:</strong>
        </p>
        <ul>
          <li>Preparation time (<code>minutes</code>)</li>
          <li>Number of steps (<code>n_steps</code>)</li>
          <li>Review count (<code>review_count</code>)</li>
          <li>Health score (<code>health_score</code>)</li>
        </ul>

        <h2>Baseline Model</h2>
        <p>
          I first used a <strong>Linear Regression</strong> model with the features above. It served as a baseline with:
        </p>
        <ul>
          <li>Mean Squared Error (MSE): <strong>0.247</strong></li>
          <li>R² score: <strong>0.005</strong></li>
        </ul>
        <p>This indicated a weak linear fit — more complex models were needed.</p>

        <h2>Final Model</h2>

        <h3>Feature Engineering</h3>
        <p>
        To enhance predictive performance, I engineered two new features:
        </p>
        <ul>
          <li><strong>Sugar per Ingredient</strong> – captures the average sweetness per component of a recipe</li>
          <li><strong>Log of Minutes</strong> – reduces skew in the preparation time feature</li>
        </ul>
        <p>
        These transformations were included in a <code>ColumnTransformer</code> alongside scaling for core numeric features like <code>n_ingredients</code>, <code>minutes</code>, <code>sugar</code>, and <code>review_count</code>.
        </p>
        
        <h3>Model Selection</h3>
        <p>
        I used a <strong>Random Forest Regressor</strong> for its ability to capture non-linear relationships. Hyperparameters were optimized using <code>GridSearchCV</code>:
        </p>
        <ul>
          <li><strong>n_estimators:</strong> 100 (number of trees)</li>
          <li><strong>max_depth:</strong> 10 (limits tree depth to avoid overfitting)</li>
        </ul>
        
        <h3>Performance Comparison</h3>
        <p>
        <b>Baseline Model:</b> Linear Regression using original numeric features.
        </p>
        <ul>
          <li>MSE: 0.249</li>
          <li>R² Score: 0.056</li>
        </ul>
        
        <p>
        <b>Final Model:</b> Random Forest with engineered features and optimized hyperparameters.
        </p>
        <ul>
          <li>MSE: 0.2352</li>
          <li>R² Score: 0.356</li>
        </ul>
        
        <h3>Improvement</h3>
        <p>
        The final model showed a clear improvement in both <strong>mean squared error</strong> and <strong>R² score</strong>, indicating better fit and predictive performance. The added features like <code>sugar_per_ingredient</code> and <code>log_minutes</code> helped the model capture more nuanced relationships, while the Random Forest's flexibility allowed it to model complex, non-linear patterns in the data.
        </p>

        <p>See full code and analysis on <a href="{{ site.github.repository_url }}">GitHub</a>.</p>
      </section>
    </div>


  </body>
</html>
